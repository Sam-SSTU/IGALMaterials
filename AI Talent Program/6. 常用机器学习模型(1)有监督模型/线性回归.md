该课程主要为大家讲授如下的内容：
- 线性回归
- 一元线性回归模型
- 损失函数
- 函数凸性
- 最小二乘法
- 梯度下降


1.  线性回归
   线性回归是用线性函数拟合自变量和因变量关系的一种算法。它是最常见的回归模型,其原理很简单,模型能力却很强,尤其是当我们把线性模型和多项式特征结合起来使用的时候,它能达到的效果是非常不错的。所以这种模型现在仍然还在使用。
   [[线性回归.png]]
2.  一元线性回归模型
	0. 三大要素：
	   1) 待求解的函数 y = a+ bx，求解出a和b之后,我们就可以根据特征x求出目标y。
	   2) 模型的目标, 要达成一个目的需要有一个目标,对于线性回归模型,我们希望函数 y 等于 a 加 b x 能够完美的拟合训练数据。 但是事实上这种情况很难做到, 所以模型的目标就形式化为使得回归函数在训练数据集上的平方误差最小。
	   3) 训练数据, 这也是所有机器学习模型都需要的内容。对于线性回归任务, 数据包含特征和标签两部分,标签需要是连续值。除了对数据的格式有要求,我们也对数据量有一些要求。线性回归模型需要的训练数据量非常小, 但是我们仍然要求数据量大于变量数,所以对于我们的一元线性回归, 训练数据就需要大于1。
	1. 损失函数
	   模型的目标是最小化函数在训练集上的平方误差, 这里我们来对模型的损失函数进行形式化。 J(a, b)表示在训练集上模型标签和预测值之间误差平方的平均值。
	   [[损失函数.png]]
	2. 函数凸性
	   凸函数的定义:在某个向量空间凸子集C上的实值函数, 对其定义在域C上任意两点 x1，x2 ，总有f(x1+x2/2)<= f(x1)+f(x2)/2
	   [[函数凸性.png]]
	3. 最小二乘法
	   求解方程的解析解, 通过微积分求解
	   线性回归目标函数
	   [[最小二乘法.png]]
	   求解上面方程组即可得出线性回归解析解。
	4. 梯度下降
	   梯度下降是一种通用的优化算法，也基本是深度学习的标准优化算法，是一种通用的优化算法，原理简单，但不保证得到最优解。可以用来解线性回归，但是实际应用中很少这么用。这里学习梯度下降是为后面的学习打基础。
	   梯度下降的步骤：
	   [[梯度下降的步骤.png]]