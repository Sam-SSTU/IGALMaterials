该课程主要为大家讲授如下的内容：
- 全连接神经网络的前向传播计算
- 全连接神经网络的反向传播计算
- Sigmoid激活函数
- 梯度消失与梯度爆炸问题
- ReLU函数
- 深度学习的基本概念


1. 全连接神经网络的前向传播计算
	全连接神经网络的前向传播计算逐层进行。每一层的计算可以记为：
	$\alpha = \sigma (z), z=\omega x + b$
	[[整个网络仅有一个神经元.png]]
2. 全连接神经网络的反向传播计算
	在全连接神经网络的反向传播计算过程中，首先通过损失函数求取输出y与期望值a的距离，损失函数记为
	$L(a,y)$
	然后根据损失函数求取模型的参数在当前状态下与最优解：
	$w,b=argminL(a,y)$
	梯度，记为：
	$dw=x·dz,db=dz|dz={\partial L(a,y)}/{\partial z}$
	最后反向传播梯度至第一层隐层，以链式求导的方式逐层向后求取权值和偏置的梯度，对第k层的梯度记为：
	$dw_{k}=x_{k}·dz_{k},db_{k}=dz_{k}|dz_{k}=\frac{{\partial a_{k}}}{{\partial z_{k}}}·\frac{{\partial z_{k+1}}}{{\partial a_{k}}}·\frac{{\partial L(a,y)}}{{\partial z_{k+1}}}$
	[[但神经元、单个数据的反向传播.png]]
	这个迭代的过程可以如下表示。
	[[梯度下降.png]]
3. Sigmoid激活函数
	Sigmoid函数是一种常见的激活函数，取值在(0,1)。
	[[Sigmoid函数的优缺点.png]]
4. 梯度消失与梯度爆炸问题
	梯度消失和梯度爆炸是深层神经网络中常见的问题。神经网络的层数越多，这两个问题就越容易出现。
	[[梯度消失与梯度爆炸.png]]
	为了应对梯度消失与梯度爆炸问题的问题，研究者们给出了如下的解决方法。
	[[梯度消失与梯度爆炸解决方法.png]]
5. ReLU函数
	ReLU函数也是一种常见的激活函数。它的计算比起Sigmoid更简单，在同等条件下消耗的算力更少。但是ReLU函数对初始值更敏感。
	[[ReLU函数.png]]
6. 深度学习的基本概念
	深度学习是通过加深神经网络的层数来自动提取数据特征（表征学习）的一种算法。这种算法能够大量节省知识依赖和人力，有效提取关键特征。
	[[深度学习.png]]